#!/usr/bin/env python
#========================================================================
# Train/Finetune for synthetic datasets generated by Bayesian/Markov networks 
#========================================================================

import sys
import os
import time
import numpy as np
import tensorflow as tf
import tensorflow.math as tfm
from tensorflow.keras import callbacks, models

import config
from datagen import synthetic_bn, synthetic_mn
from train import create_nn_model, create_pgm, model_eval

taskID=int(os.environ['SLURM_ARRAY_TASK_ID']) if 'SLURM_ARRAY_TASK_ID' in os.environ else 0

def get_pdf_dataset(dataset):
    
    xs, ys = [], []
    for input, label in dataset:
        xs.extend(input.numpy())
        ys.extend(label.numpy())

    xs, ys = np.vstack(xs), np.vstack(ys)
    dim_x = xs.shape[1]
    bits = np.array([int(2 ** i) for i in range(dim_x)][::-1], dtype='int')
    cnt, pos = np.zeros(int(2 ** dim_x), dtype='int'), np.zeros(int(2 ** dim_x))
    for x, y in zip(xs, ys):
        idx = x@bits.T
        cnt[idx] += 1
        pos[idx] += y.item()
    
    #print("The true probabilities of all inputs are ", pos / cnt)
    print(cnt)
    return pos / cnt
    
def sampling(model, test_ds, N_sampling=1000):

    '''
    Calculate predicted P(Y|X) for a model by sampling
    '''

    network, kernels = None, None
    pred_probas = []

    print("Start calculating the pdf of the finetuned PGM.")
    start_time = time.time()
    for i in range(N_sampling):
        
        network = [model.call(data) for data, target in test_ds]
        if not model.is_gibbs:
            kernels = [model.generate_hmc_kernel(data, None) for data, target in test_ds]
            # sample hidden state with HMC
            for bs, (data, target) in enumerate(test_ds):
                network = [model.propose_new_state_hamiltonian(x, net, None, ker, is_update_kernel=False) for (x, y), net, ker in zip(test_ds, network, kernels)]
        
        # get output probability
        pred_proba = []
        for bs, (data, target) in enumerate(test_ds):
            logits = model.output_layer(network[bs][-1])
            pred_proba.append(tfm.sigmoid(logits).numpy())
            
        pred_probas.append(np.concatenate(pred_proba))

    print("The sampling time is ", time.time() - start_time)
    return pred_probas, sum(pred_probas) / N_sampling

class CustomSaveCallback(callbacks.Callback):
    def __init__(self, model_save_path, res_save_path, save_epoch, final_epoch, test_ds):
        super(CustomSaveCallback, self).__init__()
        self.model_save_path = model_save_path
        self.res_save_path = res_save_path
        self.save_epoch = save_epoch
        self.final_epoch = final_epoch
        self.test_ds = test_ds
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs=None):
        if epoch + 1 == self.save_epoch:  # Epochs are zero-indexed
            elasped_time = time.time() - self.start_time
            pred_proba = self.model.predict(self.test_ds)
            self.model.save(f"{self.model_save_path}_epoch_{self.save_epoch}.h5")
            print(f"Model saved at epoch {self.save_epoch}")
            np.savez(f"{self.res_save_path}_epoch_{self.save_epoch}.npz", time=elasped_time, logs=logs, proba=pred_proba)

    def on_train_end(self, logs=None):
        elapsed_time = time.time() - self.start_time
        pred_proba = self.model.predict(self.test_ds)
        self.model.save(f"{self.model_save_path}_epoch_{self.final_epoch}.h5")
        print("Model saved at the end of training.")
        np.savez(f"{self.res_save_path}_epoch_{self.final_epoch}.npz", time=elapsed_time, logs=logs, proba=pred_proba)


def train(dat_type, k, train_ds, test_ds, input_shape, layer_sizes, final_epoch=1000, save_epoch=100, lr=1e-4):

    '''
    Train a base NN with the same number of hidden layers and more hidden nodes in each layer.
    k is used for generating the same_model and training history files.
    Learning rate is set manually. No tuning or early stopping with only train dataset.
    is_split=False means training with whole dataset and without train/test split. The test data would be all possible input settings. 
    '''

    model = create_nn_model(input_shape, layer_sizes, lr)
    model_path = config.model_path + f"{dat_type}{k}/run_{taskID}/model_{dat_type}{k}_{taskID}_bp_-4"
    res_path = config.hist_path + f"{dat_type}{k}/run_{taskID}/hist_{dat_type}{k}_{taskID}_bp_-4" # save history of training as results

    # Train the model 1000 epochs and save the model and results both at the end and at epoch 100
    custom_save_callback = CustomSaveCallback(model_save_path=model_path, res_save_path=res_path, 
                                              save_epoch=save_epoch, final_epoch=final_epoch, test_ds=test_ds)

    print("Start NN training.")
    model.fit(train_ds, epochs=final_epoch, verbose=0, callbacks=[custom_save_callback])

def finetune(dat_type, k, train_ds, test_ds, model_nn, layer_sizes, L, train_epoch, ft_final_epoch=40, ft_eval_epoch=20, lr=1e-4):

    model_pgm, kernels = create_pgm(layer_sizes, lr, L, train_ds, model_nn=model_nn)
    network = [model_pgm.call(data) for data, target in train_ds]

    ft_losses, ft_accs, ft_aucs, ft_probas = [], [], [], []

    print(f"Start HMC_{L} Finetuning.")
    start_time = time.time()
    for epoch in range(ft_final_epoch):
        for bs, (data, target) in enumerate(train_ds):
            model_pgm.update_weights(data, network[bs], target)
            if L == -1: # Gibbs
                network = [model_pgm.gibbs_new_state(x, net, y) for (x, y), net in zip(train_ds, network)]
            else:
                network = [model_pgm.propose_new_state_hamiltonian(x, net, y, ker, is_update_kernel=False) for (x, y), net, ker in zip(train_ds, network, kernels)]

        ft_loss, ft_acc, ft_auc, ft_proba = model_eval(model_pgm, train_ds)
        print("Epoch %d/%d: - %.4fs/step - ft_loss: %.4f - ft_acc: %.4f - ft_auc: %.4f" 
              % (epoch + 1, ft_final_epoch, (time.time() - start_time) / (epoch + 1), ft_loss, ft_acc, ft_auc))
         
        ft_losses.append(ft_loss.numpy())       
        ft_accs.append(ft_acc)
        ft_aucs.append(ft_auc)
        ft_probas.append(ft_proba)

        if epoch == ft_eval_epoch - 1 or epoch == ft_final_epoch - 1:

            # save the model and results at the end and at epoch 20
            ft_time = time.time() - start_time
            pgm_weights = model_pgm.get_weights()
            model_path = config.model_path + f"{dat_type}{k}/run_{taskID}/modelft_{dat_type}{k}_{taskID}_epoch_{train_epoch}_-4_hmc{L}_{epoch + 1}_-4.npz"
            np.savez(model_path, weights=pgm_weights)

            all_sampled_probs, prob_pgm = sampling(model_pgm, test_ds)
            res_path = config.res_path + f"{dat_type}{k}/run_{taskID}/res_{dat_type}{k}_{taskID}_epoch_{train_epoch}_-4_hmc{L}_{epoch + 1}_-4.npz"
            np.savez(res_path, time=ft_time, all_probs=all_sampled_probs, prob=prob_pgm)
    
    ft_hist = {"ft_acc": ft_accs, "ft_auc": ft_aucs, "ft_loss": ft_losses, "ft_proba": ft_probas}
    hist_path = config.hist_path + f"{dat_type}{k}/run_{taskID}/histft_{dat_type}{k}_{taskID}_epoch_{train_epoch}_-4_hmc{L}_{ft_final_epoch}_-4.npz"
    np.savez(hist_path, hist=ft_hist)

    return model_pgm

def main():

    random_seed = config.random_seed
    layer_sizes = config.layer_sizes
    lr_train = config.lr_train
    train_save_epoch = config.train_save_epoch
    train_final_epoch = config.train_final_epoch # final number of trainning epochs, 1000
    lr_finetune = config.lr_finetune
    ft_eval_epoch = config.ft_eval_epoch
    ft_final_epoch = config.ft_final_epoch # final number of finetuning epochs, 40

    dat_type = sys.argv[1]
    k = int(sys.argv[2])
    L = int(sys.argv[3])
    
    
    if dat_type == "bn":
        input_shape, train_ds, test_ds = synthetic_bn(k, random_seed)
    elif dat_type == "mn":
        input_shape, train_ds, test_ds = synthetic_mn(k, random_seed)
    
    model_name = config.model_path + f"{dat_type}{k}/run_{taskID}/model_{dat_type}{k}_{taskID}_bp_-4_epoch_{train_final_epoch}.h5"
    # if the model does not exist, train the two NN models with different training epochs
    if not os.path.exists(model_name):
        train(dat_type, k, train_ds, test_ds, input_shape, layer_sizes, train_final_epoch, train_save_epoch, lr_train)

    for train_epoch in [train_save_epoch, train_final_epoch]:

        # load each model and finetune
        model_name = config.model_path + f"{dat_type}{k}/run_{taskID}/model_{dat_type}{k}_{taskID}_bp_-4_epoch_{train_epoch}.h5"
        loaded_model = models.load_model(model_name)
        finetune(dat_type, k, train_ds, test_ds, loaded_model, layer_sizes, L, train_epoch, ft_final_epoch, ft_eval_epoch, lr_finetune)
    
if __name__ == "__main__":
    main()   